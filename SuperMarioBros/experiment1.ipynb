{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from  torch.cuda.amp import autocast\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "%reload_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "import gym_super_mario_bros.actions as JoypadActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.env_wrappers import EnvWrapperFactory\n",
    "from agents.ForgetfulAgent import ForgetfulAgent\n",
    "from lib.MetricLogger import MetricLogger\n",
    "%reload_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageShape = (84, 84)\n",
    "actionShape = len(JoypadActions.SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before any transformations: (240, 256, 3)\n",
      "shape after grayscaler: (240, 256)\n",
      "shape after resizer: (84, 84)\n",
      "shape after all transformations: (5, 84, 84)\n",
      "(5, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload \n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, JoypadActions.SIMPLE_MOVEMENT)\n",
    "env = EnvWrapperFactory.convert(env, shape=imageShape)\n",
    "state = env.reset()\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 84, 84)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload \n",
    "# agent = ForgetfulAgent(state_shape=env.observation_space.shape, action_shape=actionShape, device=device)\n",
    "agent = ForgetfulAgent(state_shape=env.observation_space.shape, action_shape=actionShape, device=device, net_name=\"CNN84x84\")\n",
    "save_dir = Path(\"logs\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "save_dir.mkdir(parents=True)\n",
    "logger = MetricLogger(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode: 0\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 0, 'stage': 1, 'status': 'small', 'time': 391, 'world': 1, 'x_pos': 297, 'y_pos': 79}\n",
      "starting episode: 1\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 700, 'stage': 1, 'status': 'small', 'time': 0, 'world': 1, 'x_pos': 898, 'y_pos': 132}\n",
      "starting episode: 2\n",
      "q:1.09375, loss=1.0425857305526733\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 1000, 'stage': 1, 'status': 'small', 'time': 228, 'world': 1, 'x_pos': 1408, 'y_pos': 255}\n",
      "starting episode: 3\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 213, 'world': 1, 'x_pos': 834, 'y_pos': 79}\n",
      "starting episode: 4\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 300, 'stage': 1, 'status': 'small', 'time': 378, 'world': 1, 'x_pos': 691, 'y_pos': 79}\n",
      "starting episode: 5\n",
      "q:-0.1458740234375, loss=0.7589696645736694\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 500, 'stage': 1, 'status': 'small', 'time': 323, 'world': 1, 'x_pos': 1137, 'y_pos': 251}\n",
      "starting episode: 6\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 400, 'stage': 1, 'status': 'small', 'time': 392, 'world': 1, 'x_pos': 312, 'y_pos': 79}\n",
      "starting episode: 7\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 700, 'stage': 1, 'status': 'small', 'time': 315, 'world': 1, 'x_pos': 1427, 'y_pos': 253}\n",
      "starting episode: 8\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 396, 'world': 1, 'x_pos': 1410, 'y_pos': 252}\n",
      "starting episode: 9\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 700, 'stage': 1, 'status': 'small', 'time': 375, 'world': 1, 'x_pos': 687, 'y_pos': 85}\n",
      "starting episode: 10\n",
      "q:0.732421875, loss=0.7577629089355469\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 800, 'stage': 1, 'status': 'small', 'time': 297, 'world': 1, 'x_pos': 1139, 'y_pos': 255}\n",
      "starting episode: 11\n",
      "{'coins': 4, 'flag_get': False, 'life': 255, 'score': 1500, 'stage': 1, 'status': 'small', 'time': 381, 'world': 1, 'x_pos': 711, 'y_pos': 79}\n",
      "starting episode: 12\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 900, 'stage': 1, 'status': 'small', 'time': 392, 'world': 1, 'x_pos': 279, 'y_pos': 79}\n",
      "starting episode: 13\n",
      "{'coins': 4, 'flag_get': False, 'life': 255, 'score': 2650, 'stage': 1, 'status': 'small', 'time': 96, 'world': 1, 'x_pos': 2455, 'y_pos': 254}\n",
      "starting episode: 14\n",
      "q:4.3125, loss=0.8184435367584229\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 393, 'world': 1, 'x_pos': 312, 'y_pos': 79}\n",
      "starting episode: 15\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 400, 'stage': 1, 'status': 'small', 'time': 391, 'world': 1, 'x_pos': 292, 'y_pos': 79}\n",
      "starting episode: 16\n",
      "{'coins': 3, 'flag_get': False, 'life': 255, 'score': 800, 'stage': 1, 'status': 'small', 'time': 374, 'world': 1, 'x_pos': 695, 'y_pos': 85}\n",
      "starting episode: 17\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 271, 'world': 1, 'x_pos': 805, 'y_pos': 79}\n",
      "starting episode: 18\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 700, 'stage': 1, 'status': 'small', 'time': 169, 'world': 1, 'x_pos': 1132, 'y_pos': 255}\n",
      "starting episode: 19\n",
      "q:5.01953125, loss=0.8412517309188843\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 700, 'stage': 1, 'status': 'small', 'time': 393, 'world': 1, 'x_pos': 1504, 'y_pos': 79}\n",
      "starting episode: 20\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 500, 'stage': 1, 'status': 'small', 'time': 396, 'world': 1, 'x_pos': 1421, 'y_pos': 253}\n",
      "starting episode: 21\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 393, 'world': 1, 'x_pos': 306, 'y_pos': 79}\n",
      "starting episode: 22\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 1500, 'stage': 1, 'status': 'small', 'time': 277, 'world': 1, 'x_pos': 895, 'y_pos': 79}\n",
      "starting episode: 23\n",
      "q:7.00390625, loss=1.2582255601882935\n",
      "{'coins': 3, 'flag_get': False, 'life': 255, 'score': 800, 'stage': 1, 'status': 'small', 'time': 340, 'world': 1, 'x_pos': 898, 'y_pos': 79}\n",
      "starting episode: 24\n",
      "{'coins': 3, 'flag_get': False, 'life': 255, 'score': 1100, 'stage': 1, 'status': 'small', 'time': 201, 'world': 1, 'x_pos': 852, 'y_pos': 79}\n",
      "starting episode: 25\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 375, 'world': 1, 'x_pos': 680, 'y_pos': 79}\n",
      "starting episode: 26\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 500, 'stage': 1, 'status': 'small', 'time': 379, 'world': 1, 'x_pos': 722, 'y_pos': 79}\n",
      "starting episode: 27\n",
      "q:5.76953125, loss=1.2608860731124878\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 700, 'stage': 1, 'status': 'small', 'time': 175, 'world': 1, 'x_pos': 801, 'y_pos': 79}\n",
      "starting episode: 28\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 200, 'stage': 1, 'status': 'small', 'time': 391, 'world': 1, 'x_pos': 287, 'y_pos': 79}\n",
      "starting episode: 29\n",
      "q:7.6171875, loss=0.9052731990814209\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 400, 'stage': 1, 'status': 'small', 'time': 393, 'world': 1, 'x_pos': 314, 'y_pos': 79}\n",
      "starting episode: 30\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 370, 'world': 1, 'x_pos': 715, 'y_pos': 79}\n",
      "starting episode: 31\n",
      "{'coins': 1, 'flag_get': False, 'life': 255, 'score': 400, 'stage': 1, 'status': 'small', 'time': 390, 'world': 1, 'x_pos': 294, 'y_pos': 79}\n",
      "starting episode: 32\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 400, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 1415, 'y_pos': 253}\n",
      "starting episode: 33\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 1409, 'y_pos': 252}\n",
      "starting episode: 34\n",
      "q:3.96484375, loss=0.4643435776233673\n",
      "{'coins': 3, 'flag_get': False, 'life': 255, 'score': 1200, 'stage': 1, 'status': 'small', 'time': 378, 'world': 1, 'x_pos': 680, 'y_pos': 79}\n",
      "starting episode: 35\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 600, 'stage': 1, 'status': 'small', 'time': 391, 'world': 1, 'x_pos': 296, 'y_pos': 79}\n",
      "starting episode: 36\n",
      "{'coins': 0, 'flag_get': False, 'life': 255, 'score': 1200, 'stage': 1, 'status': 'small', 'time': 290, 'world': 1, 'x_pos': 886, 'y_pos': 79}\n",
      "starting episode: 37\n",
      "{'coins': 4, 'flag_get': False, 'life': 255, 'score': 900, 'stage': 1, 'status': 'small', 'time': 363, 'world': 1, 'x_pos': 692, 'y_pos': 79}\n",
      "starting episode: 38\n",
      "q:5.76171875, loss=0.8793234825134277\n",
      "{'coins': 2, 'flag_get': False, 'life': 255, 'score': 700, 'stage': 1, 'status': 'small', 'time': 392, 'world': 1, 'x_pos': 295, 'y_pos': 85}\n",
      "starting episode: 39\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 6.91 GiB already allocated; 0 bytes free; 7.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21932\\3803209988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\RLandGameTheoryExperiments\\SuperMarioBros\\agents\\ForgetfulAgent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateTarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplayExperiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;31m#endregion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\RLandGameTheoryExperiments\\SuperMarioBros\\agents\\DQNAgent.py\u001b[0m in \u001b[0;36mreplayExperiences\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m                                     \u001b[0mnext_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                                     \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                                     \u001b[0mdones\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m                                     )\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\RLandGameTheoryExperiments\\SuperMarioBros\\agents\\DQNAgent.py\u001b[0m in \u001b[0;36mgetTargetQs\u001b[1;34m(self, rewards, next_states, actions, dones)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mnextStateQs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"online\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mbestActions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnextStateQs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# axis or dim parameter is the one to reduce. this is different from tensorflow. axis = 0 means batch dim will be reduced. It does not mean arg max will be applied on each row.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mtargetQs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestActions\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         return (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gymnes\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\RLandGameTheoryExperiments\\SuperMarioBros\\agents\\DoubleNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, model)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\gymnes\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gymnes\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gymnes\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gymnes\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gymnes\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 6.91 GiB already allocated; 0 bytes free; 7.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try training for stage 1 only\n",
    "\n",
    "maxStepsPerEpisode = 100_000_000\n",
    "learnCount = 0\n",
    "episodes = 10000\n",
    "\n",
    "for eps in range(episodes):\n",
    "    state = env.reset()\n",
    "    print(f\"starting episode: {eps}\")\n",
    "    for i in range(maxStepsPerEpisode):\n",
    "\n",
    "        action = agent.getAction(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if info[\"flag_get\"]:\n",
    "            print(f\"reached a flag\")\n",
    "            print(info)\n",
    "            reward += 1000\n",
    "\n",
    "        # add to memory\n",
    "        agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        q, loss = agent.learn()\n",
    "        \n",
    "        logger.log_step(reward, loss, q)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if q is not None:\n",
    "            learnCount += 1\n",
    "            if learnCount % 1000 == 0:\n",
    "                print(f\"q:{q}, loss={loss}\")\n",
    "        \n",
    "        # if reward < 0:\n",
    "        #     print(f\"got negative reward({reward}) with action {action}\")\n",
    "        #     print(info)\n",
    "            \n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "            \n",
    "    # print(f\"done: {done},\\n info: {info}\")\n",
    "    logger.log_episode()\n",
    "    if eps % 1 == 0:\n",
    "        logger.record(episode=eps, epsilon=agent.exploration_rate, step=agent.current_step)\n",
    "        print(info)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, JoypadActions.SIMPLE_MOVEMENT)\n",
    "env = EnvWrapperFactory.convert(env, shape=imageShape)\n",
    "\n",
    "\n",
    "done = True\n",
    "count = 0 \n",
    "for step in range(100000):\n",
    "    if done:\n",
    "        count += 1\n",
    "        if count > 2:\n",
    "            break\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(agent.getAction(state))\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCheckpoint(name, epoch, model, optimizer):\n",
    "    \n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"exploration_rate\": agent.exploration_rate\n",
    "        \n",
    "    # }, f\"{model.name}-checkpoint-{epoch}\")\n",
    "    }, f\"{name}-checkpoint-{epoch}.pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = (\n",
    "#             save_dir / f\"mario_net_{int(agent.current_step // agent.onlinePeriod)}.chkpt\"\n",
    "#         )\n",
    "# torch.save(\n",
    "#     dict(model=agent.net.state_dict(), exploration_rate=agent.exploration_rate),\n",
    "#     save_path,\n",
    "# )\n",
    "saveCheckpoint(\"ForgetfulAgent-CNN50x50\", 500, agent.net, agent.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(dir=\"\", epoch=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageShape = (84, 84)\n",
    "actionShape = len(JoypadActions.SIMPLE_MOVEMENT)\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, JoypadActions.SIMPLE_MOVEMENT)\n",
    "env = EnvWrapperFactory.convert(env, shape=imageShape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "agent = ForgetfulAgent(state_shape=env.observation_space.shape, action_shape=actionShape, device=device, net_name=\"CNN84x84\")\n",
    "agent.load(\"models/ForgetfulAgent-CNN84x84-checkpoint-Apr-08-2022-9000.pytorch\")\n",
    "\n",
    "\n",
    "done = True\n",
    "count = 0 \n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        count += 1\n",
    "        if count > 2:\n",
    "            break\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(agent.getBestAction(state))\n",
    "    print(f\"{info['x_pos']} {info['time']}\")\n",
    "    if done:\n",
    "        print(info)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
